{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cfe4269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index==0.11.11\n",
      "  Using cached llama_index-0.11.11-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llama-index-agent-openai<0.4.0,>=0.3.4 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_agent_openai-0.3.4-py3-none-any.whl.metadata (728 bytes)\n",
      "Collecting llama-index-cli<0.4.0,>=0.3.1 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_cli-0.3.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting llama-index-core<0.12.0,>=0.11.10 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_core-0.11.23-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.4 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_embeddings_openai-0.2.5-py3-none-any.whl.metadata (686 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.7.7-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting llama-index-llms-openai<0.3.0,>=0.2.9 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_llms_openai-0.2.16-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl.metadata (729 bytes)\n",
      "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting llama-index-readers-file<0.3.0,>=0.2.0 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_readers_file-0.2.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting nltk>3.8.1 (from llama-index==0.11.11)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.4.0,>=0.3.4->llama-index==0.11.11)\n",
      "  Using cached openai-1.92.2-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting PyYAML>=6.0.1 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached sqlalchemy-2.0.41-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.6 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached aiohttp-3.12.13-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fsspec>=2023.5.0 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting httpx (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in ./ch01_env/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11) (1.6.0)\n",
      "Collecting networkx>=3.0 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting numpy<2.0.0 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Collecting pillow>=9.0.0 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached pillow-11.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.0 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting requests>=2.31.0 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached tiktoken-0.9.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm<5.0.0,>=4.66.1 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./ch01_env/lib/python3.11/site-packages (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11) (4.14.0)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached wrapt-1.17.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting llama-cloud==0.1.26 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud-0.1.26-py3-none-any.whl.metadata (1.2 kB)\n",
      "INFO: pip is looking at multiple versions of llama-index-indices-managed-llama-cloud to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.7.6-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.7.5-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-cloud==0.1.25 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud-0.1.25-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.7.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-cloud==0.1.23 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud-0.1.23-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.7.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.7.2-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.7.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.7.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-cloud==0.1.21 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud-0.1.21-py3-none-any.whl.metadata (1.2 kB)\n",
      "INFO: pip is still looking at multiple versions of llama-index-indices-managed-llama-cloud to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud-0.1.29-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.10-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.9-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.8-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.5-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.6.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pandas (from llama-index-legacy<0.10.0,>=0.9.48->llama-index==0.11.11)\n",
      "  Using cached pandas-2.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index==0.11.11)\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index==0.11.11)\n",
      "  Using cached pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index==0.11.11)\n",
      "  Using cached striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "INFO: pip is looking at multiple versions of llama-index-readers-llama-parse to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-index-readers-llama-parse>=0.3.0 (from llama-index==0.11.11)\n",
      "  Using cached llama_index_readers_llama_parse-0.3.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_parse-0.6.37-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting click (from nltk>3.8.1->llama-index==0.11.11)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk>3.8.1->llama-index==0.11.11)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>3.8.1->llama-index==0.11.11)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached frozenlist-1.7.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached multidict-6.6.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached propcache-0.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached yarl-1.20.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (73 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index==0.11.11)\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting certifi>=2024.7.4 (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting anyio (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting llama-cloud-services>=0.6.37 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud_services-0.6.37-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index==0.11.11)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index==0.11.11)\n",
      "  Using cached jiter-0.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.4->llama-index==0.11.11)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached charset_normalizer-3.4.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (35 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting greenlet>=1 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached greenlet-3.2.3-cp311-cp311-macosx_11_0_universal2.whl.metadata (4.1 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./ch01_env/lib/python3.11/site-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index==0.11.11) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index==0.11.11)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index==0.11.11)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_parse-0.6.36-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.36 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud_services-0.6.36-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud-0.1.28-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_parse-0.6.35-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.35 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud_services-0.6.35-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud-0.1.27-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_parse-0.6.34-py3-none-any.whl.metadata (6.9 kB)\n",
      "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-cloud-services>=0.6.32 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud_services-0.6.34-py3-none-any.whl.metadata (3.4 kB)\n",
      "  Using cached llama_cloud_services-0.6.33-py3-none-any.whl.metadata (3.4 kB)\n",
      "  Using cached llama_cloud_services-0.6.32-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_parse-0.6.33-py3-none-any.whl.metadata (6.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached llama_parse-0.6.32-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Using cached llama_parse-0.6.31-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.31 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud_services-0.6.31-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_parse-0.6.30-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.30 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud_services-0.6.30-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_parse-0.6.28-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.28 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud_services-0.6.29-py3-none-any.whl.metadata (3.4 kB)\n",
      "  Using cached llama_cloud_services-0.6.28-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_parse-0.6.27-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.27 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud_services-0.6.27-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_parse-0.6.26-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.26 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud_services-0.6.26-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_parse-0.6.25-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.24 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud_services-0.6.25-py3-none-any.whl.metadata (3.4 kB)\n",
      "  Using cached llama_cloud_services-0.6.24-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_parse-0.6.24-py3-none-any.whl.metadata (6.9 kB)\n",
      "  Using cached llama_parse-0.6.23-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.23 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud_services-0.6.23-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud-0.1.22-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_parse-0.6.22-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.22 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud_services-0.6.22-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud-0.1.19-py3-none-any.whl.metadata (902 bytes)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_parse-0.6.21-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.21 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached llama_cloud_services-0.6.21-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=4.3.7 in ./ch01_env/lib/python3.11/site-packages (from llama-cloud-services>=0.6.21->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11) (4.3.8)\n",
      "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.21->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.3.0->llama-index==0.11.11)\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in ./ch01_env/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.10->llama-index==0.11.11) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in ./ch01_env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index==0.11.11) (1.17.0)\n",
      "Using cached llama_index-0.11.11-py3-none-any.whl (6.8 kB)\n",
      "Using cached llama_index_agent_openai-0.3.4-py3-none-any.whl (13 kB)\n",
      "Using cached llama_index_cli-0.3.1-py3-none-any.whl (27 kB)\n",
      "Using cached llama_index_core-0.11.23-py3-none-any.whl (1.6 MB)\n",
      "Using cached llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
      "Using cached llama_index_indices_managed_llama_cloud-0.6.0-py3-none-any.whl (11 kB)\n",
      "Using cached llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
      "Using cached llama_index_llms_openai-0.2.16-py3-none-any.whl (13 kB)\n",
      "Using cached llama_index_multi_modal_llms_openai-0.2.3-py3-none-any.whl (5.9 kB)\n",
      "Using cached llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
      "Using cached llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
      "Using cached llama_index_readers_file-0.2.2-py3-none-any.whl (38 kB)\n",
      "Using cached llama_index_readers_llama_parse-0.3.0-py3-none-any.whl (2.5 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached aiohttp-3.12.13-cp311-cp311-macosx_11_0_arm64.whl (469 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached llama_parse-0.6.21-py3-none-any.whl (4.9 kB)\n",
      "Using cached llama_cloud_services-0.6.21-py3-none-any.whl (37 kB)\n",
      "Using cached llama_cloud-0.1.19-py3-none-any.whl (263 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Using cached openai-1.92.2-py3-none-any.whl (753 kB)\n",
      "Using cached pillow-11.2.1-cp311-cp311-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Using cached pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached sqlalchemy-2.0.41-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Using cached striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached tiktoken-0.9.0-cp311-cp311-macosx_11_0_arm64.whl (1.0 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached wrapt-1.17.2-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached pandas-2.3.0-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp311-cp311-macosx_10_9_universal2.whl (198 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached frozenlist-1.7.0-cp311-cp311-macosx_11_0_arm64.whl (47 kB)\n",
      "Using cached greenlet-3.2.3-cp311-cp311-macosx_11_0_universal2.whl (270 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jiter-0.10.0-cp311-cp311-macosx_11_0_arm64.whl (321 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached multidict-6.6.0-cp311-cp311-macosx_11_0_arm64.whl (44 kB)\n",
      "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Using cached propcache-0.3.2-cp311-cp311-macosx_11_0_arm64.whl (43 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached yarl-1.20.1-cp311-cp311-macosx_11_0_arm64.whl (89 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: striprtf, pytz, filetype, dirtyjson, wrapt, urllib3, tzdata, typing-inspection, tqdm, tenacity, SQLAlchemy, soupsieve, sniffio, regex, PyYAML, python-dotenv, pypdf, pydantic-core, propcache, pillow, numpy, networkx, mypy-extensions, multidict, marshmallow, joblib, jiter, idna, h11, greenlet, fsspec, frozenlist, distro, click, charset_normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, requests, pydantic, pandas, nltk, httpcore, deprecated, beautifulsoup4, anyio, aiosignal, tiktoken, httpx, dataclasses-json, aiohttp, openai, llama-index-core, llama-cloud, llama-index-readers-file, llama-index-llms-openai, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.41 aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.9.0 attrs-25.3.0 beautifulsoup4-4.13.4 certifi-2025.6.15 charset_normalizer-3.4.2 click-8.2.1 dataclasses-json-0.6.7 deprecated-1.2.18 dirtyjson-1.0.8 distro-1.9.0 filetype-1.2.0 frozenlist-1.7.0 fsspec-2025.5.1 greenlet-3.2.3 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jiter-0.10.0 joblib-1.5.1 llama-cloud-0.1.19 llama-cloud-services-0.6.21 llama-index-0.11.11 llama-index-agent-openai-0.3.4 llama-index-cli-0.3.1 llama-index-core-0.11.23 llama-index-embeddings-openai-0.2.5 llama-index-indices-managed-llama-cloud-0.6.0 llama-index-legacy-0.9.48.post4 llama-index-llms-openai-0.2.16 llama-index-multi-modal-llms-openai-0.2.3 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.2 llama-index-readers-llama-parse-0.3.0 llama-parse-0.6.21 marshmallow-3.26.1 multidict-6.6.0 mypy-extensions-1.1.0 networkx-3.5 nltk-3.9.1 numpy-1.26.4 openai-1.92.2 pandas-2.3.0 pillow-11.2.1 propcache-0.3.2 pydantic-2.11.7 pydantic-core-2.33.2 pypdf-4.3.1 python-dotenv-1.1.1 pytz-2025.2 regex-2024.11.6 requests-2.32.4 sniffio-1.3.1 soupsieve-2.7 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.9.0 tqdm-4.67.1 typing-inspect-0.9.0 typing-inspection-0.4.1 tzdata-2025.2 urllib3-2.5.0 wrapt-1.17.2 yarl-1.20.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index==0.11.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d4e1799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430f5f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd0d1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작가는 대학 전에 글쓰기와 프로그래밍을 주요 관심사로 삼았습니다. 중학교 9학년 때 IBM 1401 컴퓨터를 사용하여 프로그래밍을 시작했고, 이후에는 마이크로컴퓨터로 전환되면서 TRS-80 컴퓨터를 구입하여 프로그래밍을 시작했습니다. 대학에서는 철학을 전공할 계획이었지만, 철학 수업이 지루해서 AI로 전과하게 되었습니다. 이후에는 컴퓨터 과학 박사과정에 진학하였고, 립스 프로그래밍에 열정을 쏟으며 논문을 쓰게 되었습니다. 그리고 예술 학교에 지원하여 미술을 공부하기 시작했습니다.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"작가의 청소년 시절은 어떠했는지 한글로 설명해 줘\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac53f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "    response = query_engine.query(\"작가의 청소년 시절은 어떠했는지 한글로 설명해 줘\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ch01_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
